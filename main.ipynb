{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b53e65e",
   "metadata": {},
   "source": [
    "## **LangChain -  FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389730da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load PDF\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split into chunks \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb70ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-A) Create Embeddings - Sentenace Transformer Embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name='all-MiniLM-L12-v2', model_kwargs={'device':'cuda'})\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Store Vector Embeddings\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "vectordb.save_local(\"faiss_all_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Retrieve \n",
    "retrieve = vectordb.as_retriever(search_type = 'similarity', search_kwargs={\"k\":10})\n",
    "retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# ---------- Load env var ----------\n",
    "load_dotenv()\n",
    "\n",
    "API_KEYS = os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\")\n",
    "API_KEYS = [k.strip() for k in API_KEYS if k.strip()]\n",
    "current_key_index = 0\n",
    "ACCESS_KEY = os.getenv(\"APP_ACCESS_KEY\")\n",
    "\n",
    "if not API_KEYS:\n",
    "    print(\"❌ No Gemini API keys found. Check .env file.\")\n",
    "    raise Exception(\"Missing GEMINI_API_KEYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "If the answer is not in the documents, say \"I don’t know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53238aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_KEY_KEY', '')\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = retrieve,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs = {\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Ask\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa.run(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e793514",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Why Adam was named Adam? Explain in details\"\n",
    "res = qa.run(q1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c9320",
   "metadata": {},
   "source": [
    "## **Pinecone Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e52e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f643b",
   "metadata": {},
   "source": [
    "### **Vectorinze Single PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63713a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------- begin of pipelines ----------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# load pdf\u001b[39;00m\n\u001b[32m      3\u001b[39m pdf_path = \u001b[33m'\u001b[39m\u001b[33mHayat-al-Qulub-Vol-1.pdf\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loader = \u001b[43mPyMuPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m docs = loader.load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:822\u001b[39m, in \u001b[36mPyMuPDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, password, mode, pages_delimiter, extract_images, images_parser, images_inner_format, extract_tables, headers, extract_tables_settings, **kwargs)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmode must be single or page\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28mself\u001b[39m.parser = PyMuPDFParser(\n\u001b[32m    824\u001b[39m     password=password,\n\u001b[32m    825\u001b[39m     mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m     extract_tables_settings=extract_tables_settings,\n\u001b[32m    833\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:140\u001b[39m, in \u001b[36mBasePDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, headers)\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(\u001b[38;5;28mself\u001b[39m.file_path):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not a valid file or url\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.file_path)\n",
      "\u001b[31mValueError\u001b[39m: File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-al-Qulub-Vol-1.pdf'\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c63413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d73ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - FAISS\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "vector_db.save_local(\"Hayat-Qulub-Alama-Majlisi-faiss-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4effc16",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae29ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 documents\n",
      "Splitting 0 documents into chunks...\n",
      "splitted 0 documents Successfully! \n",
      "Loaded 2547 chunks from 0 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 2547\n",
      "Loaded 1 documents\n",
      "Splitting 1 documents into chunks...\n",
      "splitted 1 documents Successfully! \n",
      "Loaded 3806 chunks from 1 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 6353\n",
      "Loaded 2 documents\n",
      "Splitting 2 documents into chunks...\n",
      "splitted 2 documents Successfully! \n",
      "Loaded 1205 chunks from 2 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 7558\n"
     ]
    }
   ],
   "source": [
    "#  Store all 3 PDFs in vector database - Pinecone\n",
    "pdfs = [\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf'\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for i, pdf in enumerate(pdfs):\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {i} documents\")\n",
    "    \n",
    "    # split text into chunks\n",
    "    print(f\"Splitting {i} documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"splitted {i} documents Successfully! \")\n",
    "    print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"Chunk list filled\")\n",
    "    print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - Pinecone\n",
    "vector_db = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e534d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8115a6b",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs - functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_into_chunks(pdfs):\n",
    "    pdfs = list(pdfs)\n",
    "    all_chunks = []\n",
    "    for i, pdf in enumerate(pdfs):\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {i} documents\")\n",
    "        \n",
    "        # split text into chunks\n",
    "        print(f\"Splitting {i} documents into chunks...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"splitted {i} documents Successfully! \")\n",
    "        print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Chunk list filled\")\n",
    "        print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def store_chunks_in_pinecone(all_chunks, embedding_model, INDEX_NAME):\n",
    "    # create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model, model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "    print(f\"Created embeddings using model: {embedding_model}\")\n",
    "    \n",
    "    # Store in vector database - Pinecone\n",
    "    pinecone_store = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "    print(f\"Stored {len(all_chunks)} chunks in Pinecone index: {INDEX_NAME}\")\n",
    "    return pinecone_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2c22",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87829124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "pinecone_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings)\n",
    "\n",
    "# Create retriever from Pinecone\n",
    "retrieve = pinecone_store.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "You have to understand the context deeply and answer accordingly.\n",
    "If the answer is not in the documents, say \"I don't know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name='gpt-4o', \n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv('OPEN_AI_KEY')\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "input = {\"context\": retrieve | format_docs, \n",
    "      \"question\": RunnablePassthrough()\n",
    "      }\n",
    "\n",
    "\n",
    "qa_chain = ( input| prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab547dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the tapestry of Islamic history, the naming of Adam is a tale woven with profound meaning.\n",
      "\n",
      "It is narrated through authentic chains that Imam Muhammad al-Baqir and Ja‘far as-Sadiq (peace be upon them) said, \"Adam was named ‘Adam’ because he was ‘Adeemul Arz’, that is he was created from the face of the earth (from dust).\"\n",
      "\n",
      "So, Adam's name is intrinsically linked to his origin, a reminder of the humble earth from which he was formed.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa_chain.invoke(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadac881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aapka sawaal Hayat-ul-Qulub ke baare mein hai. Chaliye, main aapko iske baare mein tafseel se batata hoon:\n",
      "\n",
      "**Hayat-ul-Qulub kisne likhi hai?**\n",
      "\n",
      "Hayat-ul-Qulub Allama Muhammad Baqir Majlisi (Rehmatullah Alaih) ne likhi hai. Allama Majlisi ek mashhoor Islamic scholar aur Muhaddith the.\n",
      "\n",
      "**Ye kitab kiske baare mein hai?**\n",
      "\n",
      "Ye kitab Ambiya (Prophets), Aimma (Imams), aur deegar buzurg hastiyon ke ahwaal (life events) aur fazail (virtues) par mabni hai. Isme unke zindagi ke waqiyat, unke akhlaq, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "**Is kitab ki kitni jildhein (volumes) hain?**\n",
      "\n",
      "Hayat-ul-Qulub ki teen jildhein hain. Har jildh mein mukhtalif anbiya aur aimma ke baare mein tafseeli malumaat hain.\n",
      "\n",
      "**Har jildh ka mukhtasar khulasa (brief summary):**\n",
      "\n",
      "*   **Jildh 1:** Is jildh mein Ambiya-e-Kiram (Prophets) ke ahwaal bayan kiye gaye hain, jaise Hazrat Adam (A.S.), Hazrat Nuh (A.S.), Hazrat Ibrahim (A.S.), Hazrat Musa (A.S.), aur Hazrat Isa (A.S.). Har Nabi ki zindagi ke aham waqiyat aur unki ummat ke liye unke paighamat ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 2:** Is jildh mein Khatim-ul-Anbiya, Hazrat Muhammad Mustafa (S.A.W.) ki zindagi ke ahwaal bayan kiye gaye hain. Aapki wiladat se lekar aapki wafaat tak ke tamam aham waqiyat, aapke akhlaq, aapke mojizaat, aur aapki ummat ke liye aapki nasihaton ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 3:** Is jildh mein Aimma-e-Ahle Bait (A.S.) ke ahwaal bayan kiye gaye hain, jaise Imam Ali (A.S.), Imam Hasan (A.S.), Imam Hussain (A.S.), aur baqi tamam Aimma (A.S.). Har Imam ki zindagi ke aham waqiyat, unke fazail, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "Hayat-ul-Qulub ek behtareen kitab hai jo Ambiya aur Aimma ke baare mein malumaat haasil karne ke liye nihayat mufeed hai. Is kitab ko padhne se insaan ke imaan mein izafa hota hai aur usko apni zindagi ko behtar tareeqe se guzarne ki hidayat milti hai.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"\"\"\n",
    "Kisne likh hai Hayatal Qulub? kiske bare me hai ye kitab? \n",
    "kitni jildhe hai iske aur kya har ek jildh ko short me smjha sakte ho?\n",
    "\"\"\"\n",
    "res2 = qa_chain.invoke(query1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81957866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check for any embedding wrapper you plan to use\n",
    "vec = embeddings.embed_query(\"test\")\n",
    "print(\"type:\", type(vec))\n",
    "print(\"len:\", len(vec))   # MUST be 1024 to match your Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52496e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "391fab36",
   "metadata": {},
   "source": [
    "## **Sulaym ibne Qays RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f0de4",
   "metadata": {},
   "source": [
    "### **RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b595684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabc29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9add8e57",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8ee82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea318175",
   "metadata": {},
   "source": [
    "## **LangChain Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ba237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant setup\n",
    "qdrant_client = QdrantClient(\n",
    "    host=\"xyz-example.eu-central.aws.cloud.qdrant.io\",\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65138dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cded5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L12-v2\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3b560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vectordb and store embeddings\n",
    "vector_db_qdrant = qdrant_client.recreate_collection(\n",
    "    collection_name=\"hayat_qulub\",\n",
    "    vectors_config=Qdrant.VectorParams(\n",
    "         size = 768,\n",
    "        distance=Qdrant.Distance.COSINE\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de4611",
   "metadata": {},
   "source": [
    "## **LlamaIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader \n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader('pdf_docs').load_data()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e58299",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbedding(model_name=\"all-MiniLM-L12-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(index_store=embeddings)\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
