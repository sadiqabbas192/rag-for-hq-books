{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b53e65e",
   "metadata": {},
   "source": [
    "## **LangChain -  FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389730da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load PDF\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split into chunks \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb70ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-A) Create Embeddings - Sentenace Transformer Embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name='all-MiniLM-L12-v2', model_kwargs={'device':'cuda'})\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Store Vector Embeddings\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "vectordb.save_local(\"faiss_all_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Retrieve \n",
    "retrieve = vectordb.as_retriever(search_type = 'similarity', search_kwargs={\"k\":10})\n",
    "retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# ---------- Load env var ----------\n",
    "load_dotenv()\n",
    "\n",
    "API_KEYS = os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\")\n",
    "API_KEYS = [k.strip() for k in API_KEYS if k.strip()]\n",
    "current_key_index = 0\n",
    "ACCESS_KEY = os.getenv(\"APP_ACCESS_KEY\")\n",
    "\n",
    "if not API_KEYS:\n",
    "    print(\"❌ No Gemini API keys found. Check .env file.\")\n",
    "    raise Exception(\"Missing GEMINI_API_KEYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "If the answer is not in the documents, say \"I don’t know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53238aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_KEY_KEY', '')\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = retrieve,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs = {\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Ask\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa.run(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e793514",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Why Adam was named Adam? Explain in details\"\n",
    "res = qa.run(q1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c9320",
   "metadata": {},
   "source": [
    "## **Pinecone Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e52e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f643b",
   "metadata": {},
   "source": [
    "### **Vectorinze Single PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63713a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------- begin of pipelines ----------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# load pdf\u001b[39;00m\n\u001b[32m      3\u001b[39m pdf_path = \u001b[33m'\u001b[39m\u001b[33mHayat-al-Qulub-Vol-1.pdf\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loader = \u001b[43mPyMuPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m docs = loader.load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:822\u001b[39m, in \u001b[36mPyMuPDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, password, mode, pages_delimiter, extract_images, images_parser, images_inner_format, extract_tables, headers, extract_tables_settings, **kwargs)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmode must be single or page\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28mself\u001b[39m.parser = PyMuPDFParser(\n\u001b[32m    824\u001b[39m     password=password,\n\u001b[32m    825\u001b[39m     mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m     extract_tables_settings=extract_tables_settings,\n\u001b[32m    833\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:140\u001b[39m, in \u001b[36mBasePDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, headers)\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(\u001b[38;5;28mself\u001b[39m.file_path):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not a valid file or url\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.file_path)\n",
      "\u001b[31mValueError\u001b[39m: File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-al-Qulub-Vol-1.pdf'\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c63413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d73ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - FAISS\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "vector_db.save_local(\"Hayat-Qulub-Alama-Majlisi-faiss-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4effc16",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae29ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 documents\n",
      "Splitting 0 documents into chunks...\n",
      "splitted 0 documents Successfully! \n",
      "Loaded 2547 chunks from 0 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 2547\n",
      "Loaded 1 documents\n",
      "Splitting 1 documents into chunks...\n",
      "splitted 1 documents Successfully! \n",
      "Loaded 3806 chunks from 1 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 6353\n",
      "Loaded 2 documents\n",
      "Splitting 2 documents into chunks...\n",
      "splitted 2 documents Successfully! \n",
      "Loaded 1205 chunks from 2 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 7558\n"
     ]
    }
   ],
   "source": [
    "#  Store all 3 PDFs in vector database - Pinecone\n",
    "pdfs = [\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf'\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for i, pdf in enumerate(pdfs):\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {i} documents\")\n",
    "    \n",
    "    # split text into chunks\n",
    "    print(f\"Splitting {i} documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"splitted {i} documents Successfully! \")\n",
    "    print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"Chunk list filled\")\n",
    "    print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - Pinecone\n",
    "vector_db = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e534d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8115a6b",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs - functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_into_chunks(pdfs):\n",
    "    pdfs = list(pdfs)\n",
    "    all_chunks = []\n",
    "    for i, pdf in enumerate(pdfs):\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {i} documents\")\n",
    "        \n",
    "        # split text into chunks\n",
    "        print(f\"Splitting {i} documents into chunks...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"splitted {i} documents Successfully! \")\n",
    "        print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Chunk list filled\")\n",
    "        print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def store_chunks_in_pinecone(all_chunks, embedding_model, INDEX_NAME):\n",
    "    # create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model, model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "    print(f\"Created embeddings using model: {embedding_model}\")\n",
    "    \n",
    "    # Store in vector database - Pinecone\n",
    "    pinecone_store = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "    print(f\"Stored {len(all_chunks)} chunks in Pinecone index: {INDEX_NAME}\")\n",
    "    return pinecone_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2c22",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87829124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "pinecone_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings)\n",
    "\n",
    "# Create retriever from Pinecone\n",
    "retrieve = pinecone_store.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "You have to understand the context deeply and answer accordingly.\n",
    "If the answer is not in the documents, say \"I don't know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name='gpt-4o', \n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv('OPEN_AI_KEY')\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "input = {\"context\": retrieve | format_docs, \n",
    "      \"question\": RunnablePassthrough()\n",
    "      }\n",
    "\n",
    "\n",
    "qa_chain = ( input| prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab547dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the tapestry of Islamic history, the naming of Adam is a tale woven with profound meaning.\n",
      "\n",
      "It is narrated through authentic chains that Imam Muhammad al-Baqir and Ja‘far as-Sadiq (peace be upon them) said, \"Adam was named ‘Adam’ because he was ‘Adeemul Arz’, that is he was created from the face of the earth (from dust).\"\n",
      "\n",
      "So, Adam's name is intrinsically linked to his origin, a reminder of the humble earth from which he was formed.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa_chain.invoke(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadac881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aapka sawaal Hayat-ul-Qulub ke baare mein hai. Chaliye, main aapko iske baare mein tafseel se batata hoon:\n",
      "\n",
      "**Hayat-ul-Qulub kisne likhi hai?**\n",
      "\n",
      "Hayat-ul-Qulub Allama Muhammad Baqir Majlisi (Rehmatullah Alaih) ne likhi hai. Allama Majlisi ek mashhoor Islamic scholar aur Muhaddith the.\n",
      "\n",
      "**Ye kitab kiske baare mein hai?**\n",
      "\n",
      "Ye kitab Ambiya (Prophets), Aimma (Imams), aur deegar buzurg hastiyon ke ahwaal (life events) aur fazail (virtues) par mabni hai. Isme unke zindagi ke waqiyat, unke akhlaq, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "**Is kitab ki kitni jildhein (volumes) hain?**\n",
      "\n",
      "Hayat-ul-Qulub ki teen jildhein hain. Har jildh mein mukhtalif anbiya aur aimma ke baare mein tafseeli malumaat hain.\n",
      "\n",
      "**Har jildh ka mukhtasar khulasa (brief summary):**\n",
      "\n",
      "*   **Jildh 1:** Is jildh mein Ambiya-e-Kiram (Prophets) ke ahwaal bayan kiye gaye hain, jaise Hazrat Adam (A.S.), Hazrat Nuh (A.S.), Hazrat Ibrahim (A.S.), Hazrat Musa (A.S.), aur Hazrat Isa (A.S.). Har Nabi ki zindagi ke aham waqiyat aur unki ummat ke liye unke paighamat ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 2:** Is jildh mein Khatim-ul-Anbiya, Hazrat Muhammad Mustafa (S.A.W.) ki zindagi ke ahwaal bayan kiye gaye hain. Aapki wiladat se lekar aapki wafaat tak ke tamam aham waqiyat, aapke akhlaq, aapke mojizaat, aur aapki ummat ke liye aapki nasihaton ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 3:** Is jildh mein Aimma-e-Ahle Bait (A.S.) ke ahwaal bayan kiye gaye hain, jaise Imam Ali (A.S.), Imam Hasan (A.S.), Imam Hussain (A.S.), aur baqi tamam Aimma (A.S.). Har Imam ki zindagi ke aham waqiyat, unke fazail, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "Hayat-ul-Qulub ek behtareen kitab hai jo Ambiya aur Aimma ke baare mein malumaat haasil karne ke liye nihayat mufeed hai. Is kitab ko padhne se insaan ke imaan mein izafa hota hai aur usko apni zindagi ko behtar tareeqe se guzarne ki hidayat milti hai.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"\"\"\n",
    "Kisne likh hai Hayatal Qulub? kiske bare me hai ye kitab? \n",
    "kitni jildhe hai iske aur kya har ek jildh ko short me smjha sakte ho?\n",
    "\"\"\"\n",
    "res2 = qa_chain.invoke(query1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81957866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check for any embedding wrapper you plan to use\n",
    "vec = embeddings.embed_query(\"test\")\n",
    "print(\"type:\", type(vec))\n",
    "print(\"len:\", len(vec))   # MUST be 1024 to match your Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52496e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "391fab36",
   "metadata": {},
   "source": [
    "## **Sulaym ibne Qays RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f0de4",
   "metadata": {},
   "source": [
    "### **RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b595684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ\")\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccabc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_into_chunks(pdfs: list):\n",
    "    print(f'Count of Documents - {len(pdfs)}')\n",
    "\n",
    "    all_chunks = []\n",
    "    for i, pdf in enumerate(pdfs):\n",
    "        i=+1\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {i} document successfully\")\n",
    "\n",
    "        # split into chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"Splitted {i} documment successfully!\")\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "\n",
    "    return all_chunks\n",
    "    \n",
    "def store_chunks_in_pinecone(all_chunks, embedding_model, INDEX_NAME):\n",
    "     #create embeddings\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name = 'BAAI/bge-large-en-v1.5', model_kwargs={\"device\": \"cuda\"} \n",
    "        )\n",
    "    print(f\"Created embeddings using model: {embedding_model}\")\n",
    "\n",
    "    # Store in vector database - Pinecone\n",
    "    pinecone_store = PineconeVectorStore.from_documents(\n",
    "        documents=all_chunks,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    print(f\"Stored {len(all_chunks)} chunks in Pinecone index: {INDEX_NAME}\")\n",
    "    return pinecone_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc478a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Documents - 1\n",
      "Loaded 1 document successfully\n",
      "Splitted 1 documment successfully!\n",
      "Total chunks so far: 753\n"
     ]
    }
   ],
   "source": [
    "chunks = process_pdf_into_chunks(pdfs=['kitab_sulaim_ibn_qays_al-hilaali.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90bc1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings using model: BAAI/bge-large-en-v1.5\n",
      "Stored 753 chunks in Pinecone index: sulaym-ibne-qays\n"
     ]
    }
   ],
   "source": [
    "vector_store = store_chunks_in_pinecone(chunks, 'BAAI/bge-large-en-v1.5', INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add8e57",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faa8ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "pinecone_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings)\n",
    "\n",
    "# Create retriever from Pinecone\n",
    "retrieve = pinecone_store.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = system_prompt = \"\"\"You are a knowledgeable assistant specializing in the book \"Sulaym ibn Qays\" and early Islamic history from a Shia perspective.\n",
    "\n",
    "Your expertise includes the Fourteen Infallibles (Ma'sumin):\n",
    "1. Prophet Muhammad (PBUH) - Mustafa\n",
    "2. Fatimah (SA) - Zahra\n",
    "3. Imam Ali (AS) - Ameerul Momeneen\n",
    "4. Imam Hasan (AS) - Mujtaba\n",
    "5. Imam Husain (AS) - Shaheed-e-Karbala\n",
    "6. Imam Ali ibn Husain (AS) - Zain-ul-Abideen\n",
    "7. Imam Muhammad ibn Ali (AS) - Baqir\n",
    "8. Imam Ja'far ibn Muhammad (AS) - Sadiq\n",
    "9. Imam Musa ibn Ja'far (AS) - Kazim\n",
    "10. Imam Ali ibn Musa (AS) - Reza\n",
    "11. Imam Muhammad ibn Ali (AS) - Taqi\n",
    "12. Imam Ali ibn Muhammad (AS) - Naqi\n",
    "13. Imam Hasan ibn Ali (AS) - Askari\n",
    "14. Imam Muhammad ibn al-Hasan (AS) - Mahdi\n",
    "\n",
    "Guidelines for your responses:\n",
    "1. **Equal Focus**: Treat all fourteen Ma'sumin with equal importance and attention when answering questions about any of them\n",
    "2. **Accuracy First**: Base your answers strictly on the provided context from Kitab Sulaym ibn Qays\n",
    "3. **Respectful Tone**: Always use appropriate respectful titles:\n",
    "   - (PBUH) or (peace be upon him) for the Prophet\n",
    "   - (SA) or (peace be upon her) for Fatimah Zahra\n",
    "   - (AS) or (peace be upon him) for the Twelve Imams\n",
    "4. **Storytelling Approach**: Present information in an engaging, narrative style that helps readers understand the historical context and spiritual significance\n",
    "5. **Cite Sources**: When possible, reference which narration, hadith number, or section your answer comes from\n",
    "6. **Acknowledge Limitations**: If the answer isn't in the provided context, clearly state: \"Based on the sections of Kitab Sulaym ibn Qays provided to me, I don't have information about this specific question.\"\n",
    "7. **Historical Context**: Explain the background, circumstances, and significance of events, not just the bare facts\n",
    "8. **Avoid Speculation**: Do not add information beyond what's in the context, even if you have general knowledge about Islamic history\n",
    "9. **Recognize All Names**: Be aware that the Ma'sumin may be referred to by their given names, titles (like Ameerul Momeneen, Zahra, Sadiq), or kunyah (like Abu al-Hasan)\n",
    "\n",
    "Context from Kitab Sulaym ibn Qays:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name='gpt-4o', \n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv('OPEN_AI_KEY')\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "input = {\"context\": retrieve | format_docs, \n",
    "      \"question\": RunnablePassthrough()\n",
    "      }\n",
    "\n",
    "\n",
    "qa_chain = ( input| prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e4d0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text from Kitab Sulaym ibn Qays, the following details are given regarding the events surrounding the martyrdom of Lady Fatimah (SA):\n",
      "\n",
      "After the Prophet Muhammad (PBUH) passed away, there was a dispute over leadership. When Ali (AS) was being forced to pledge allegiance to Abu Bakr, Qunfuz, acting on Umar's orders, physically assaulted Lady Fatimah (SA). The text states that Qunfuz hit Lady Fatimah (SA) with a whip when she came between Ali (AS) and the people. Umar had instructed Qunfuz to hit her if she intervened. As a result, Qunfuz forced her behind the door and pushed it, causing her rib to break and leading to a miscarriage. The text indicates that Lady Fatimah (SA) remained ill due to this incident until she passed away as a martyr.\n",
      "\n",
      "The text also mentions that when Lady Fatimah (SA) was visited by Abu Bakr and Umar to seek forgiveness, she reminded them that she had heard the Holy Prophet (PBUH) say, \"Fatimah is a piece of mine; whoever hurts her has hurt me.\" She then raised her hands and said, \"O Allah, these two have hurt me. I am complaining about these two to you and to your Messenger. By God, I cannot ever be happy with you two until I meet my father, the Messenger of Allah, and tell him whatever you two have done. They will decide concerning you both.\"\n",
      "\n",
      "The text also mentions that Ali (AS) recited the five prayers in the mosque. When he was going to recite prayers Abu Bakr and Umar used to ask him: (\"How is the daughter of the Messenger of Allah?\") until her illness increased. They both asked him about her and said: (\"You know what happened between her and us, so if you feel it appropriate, permit us to go to her and apologize to her for our sin.\") He (a.s) said: [\"This is in your hands.\"]\n",
      "\n",
      "Finally, the text mentions that when Lady Fatimah (SA) passed away, Ali (AS) made sure that Abu Bakr and Umar did not lead the prayers at her funeral, fulfilling her will.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Who killed Lady Fatimah (SA)? Explain in detail how all that happened.\"\n",
    "res = qa_chain.invoke(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea318175",
   "metadata": {},
   "source": [
    "## **LangChain Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628e6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5ba237",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QdrantClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# qdrant setup\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m qdrant_client = \u001b[43mQdrantClient\u001b[49m(\n\u001b[32m      3\u001b[39m     host=\u001b[33m\"\u001b[39m\u001b[33mxyz-example.eu-central.aws.cloud.qdrant.io\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mQDRANT_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'QdrantClient' is not defined"
     ]
    }
   ],
   "source": [
    "# qdrant setup\n",
    "qdrant_client = QdrantClient(\n",
    "    host=\"xyz-example.eu-central.aws.cloud.qdrant.io\",\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65138dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cded5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L12-v2\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3b560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vectordb and store embeddings\n",
    "vector_db_qdrant = qdrant_client.recreate_collection(\n",
    "    collection_name=\"hayat_qulub\",\n",
    "    vectors_config=Qdrant.VectorParams(\n",
    "         size = 768,\n",
    "        distance=Qdrant.Distance.COSINE\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de4611",
   "metadata": {},
   "source": [
    "## **LlamaIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader \n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader('pdf_docs').load_data()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e58299",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbedding(model_name=\"all-MiniLM-L12-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(index_store=embeddings)\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
