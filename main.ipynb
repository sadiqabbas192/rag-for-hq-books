{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "389730da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ab394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load PDF\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b67dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.pdf.PyPDFLoader object at 0x00000258CF9536F0>\n"
     ]
    }
   ],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "024ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb70ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\AppData\\Local\\Temp\\ipykernel_14144\\4218718545.py:2: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(model_name='all-MiniLM-L12-v2', model_kwargs={'device':'cuda'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L12-v2', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-A) Create Embeddings - Sentenace Transformer Embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name='all-MiniLM-L12-v2', model_kwargs={'device':'cuda'})\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5afc42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Store Vector Embeddings\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "vectordb.save_local(\"faiss_all_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025908492900>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Retrieve \n",
    "retrieve = vectordb.as_retriever(search_type = 'similarity', search_kwargs={\"k\":10})\n",
    "retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# ---------- Load env var ----------\n",
    "load_dotenv()\n",
    "\n",
    "API_KEYS = os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\")\n",
    "API_KEYS = [k.strip() for k in API_KEYS if k.strip()]\n",
    "current_key_index = 0\n",
    "ACCESS_KEY = os.getenv(\"APP_ACCESS_KEY\")\n",
    "\n",
    "if not API_KEYS:\n",
    "    print(\"❌ No Gemini API keys found. Check .env file.\")\n",
    "    raise Exception(\"Missing GEMINI_API_KEYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eeb85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "If the answer is not in the documents, say \"I don’t know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53238aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_KEY_KEY', 'AIzaSyB2QeHIovOQPyVuG5AUjT-CRHftSaCJvGA')\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = retrieve,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs = {\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Ask\n",
    "\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa.run(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e793514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beautiful tapestry of creation, the naming of our father Adam (peace be upon him) holds a profound significance, rooted in the very essence of his being.\n",
      "\n",
      "According to authentic narrations from revered Imams, Muhammad al-Baqir and Ja‘far as-Sadiq (peace be upon them), Adam was named 'Adam' because he was **'Adeemul Arz'**, which means he was created from the very face of the earth, from its dust. Another perspective suggests that 'Adeemul Arz' refers specifically to the fourth layer of the earth.\n",
      "\n",
      "This understanding is further illuminated by a report where ‘Abdullah bin Salaam inquired of the noble Messenger of Allah (peace and blessings be upon him) about the reason for Adam's name. The Prophet (peace and blessings be upon him) confirmed that it was indeed because Adam was fashioned from the dust of the Earth.\n",
      "\n",
      "When ‘Abdullah then asked if Adam was created from dust of a single location or a mixture, the Prophet (peace and blessings be upon him) revealed a beautiful detail: \"The dust resembles him most because it is white, red, green, pink and blue. It has sweet, sour tastes, agreeable and disagree-\" This indicates that Adam, and by extension all of humanity, was created from a diverse collection of earth, reflecting the vast array of colors, temperaments, and qualities found within humankind.\n",
      "\n",
      "Thus, the name 'Adam' serves as a timeless reminder of our humble origins, connecting us directly to the very soil from which we were formed by the Almighty's magnificent power.\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Why Adam was named Adam? Explain in details\"\n",
    "res = qa.run(q1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ced6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6352d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e6d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cded5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c974c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
