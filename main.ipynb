{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b53e65e",
   "metadata": {},
   "source": [
    "## **LangChain -  FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389730da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load PDF\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split into chunks \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb70ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-A) Create Embeddings - Sentenace Transformer Embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name='all-MiniLM-L12-v2', model_kwargs={'device':'cuda'})\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Store Vector Embeddings\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "vectordb.save_local(\"faiss_all_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Retrieve \n",
    "retrieve = vectordb.as_retriever(search_type = 'similarity', search_kwargs={\"k\":10})\n",
    "retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# ---------- Load env var ----------\n",
    "load_dotenv()\n",
    "\n",
    "API_KEYS = os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\")\n",
    "API_KEYS = [k.strip() for k in API_KEYS if k.strip()]\n",
    "current_key_index = 0\n",
    "ACCESS_KEY = os.getenv(\"APP_ACCESS_KEY\")\n",
    "\n",
    "if not API_KEYS:\n",
    "    print(\"❌ No Gemini API keys found. Check .env file.\")\n",
    "    raise Exception(\"Missing GEMINI_API_KEYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "If the answer is not in the documents, say \"I don’t know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53238aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_KEY_KEY', '')\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = retrieve,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs = {\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Ask\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa.run(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e793514",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Why Adam was named Adam? Explain in details\"\n",
    "res = qa.run(q1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c9320",
   "metadata": {},
   "source": [
    "## **Pinecone Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e52e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f643b",
   "metadata": {},
   "source": [
    "### **Vectorinze Single PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63713a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------- begin of pipelines ----------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# load pdf\u001b[39;00m\n\u001b[32m      3\u001b[39m pdf_path = \u001b[33m'\u001b[39m\u001b[33mHayat-al-Qulub-Vol-1.pdf\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loader = \u001b[43mPyMuPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m docs = loader.load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:822\u001b[39m, in \u001b[36mPyMuPDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, password, mode, pages_delimiter, extract_images, images_parser, images_inner_format, extract_tables, headers, extract_tables_settings, **kwargs)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmode must be single or page\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28mself\u001b[39m.parser = PyMuPDFParser(\n\u001b[32m    824\u001b[39m     password=password,\n\u001b[32m    825\u001b[39m     mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m     extract_tables_settings=extract_tables_settings,\n\u001b[32m    833\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:140\u001b[39m, in \u001b[36mBasePDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, headers)\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(\u001b[38;5;28mself\u001b[39m.file_path):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not a valid file or url\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.file_path)\n",
      "\u001b[31mValueError\u001b[39m: File path Hayat-al-Qulub-Vol-1.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-al-Qulub-Vol-1.pdf'\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c63413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d73ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - FAISS\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "vector_db.save_local(\"Hayat-Qulub-Alama-Majlisi-faiss-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4effc16",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae29ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 documents\n",
      "Splitting 0 documents into chunks...\n",
      "splitted 0 documents Successfully! \n",
      "Loaded 2547 chunks from 0 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 2547\n",
      "Loaded 1 documents\n",
      "Splitting 1 documents into chunks...\n",
      "splitted 1 documents Successfully! \n",
      "Loaded 3806 chunks from 1 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 6353\n",
      "Loaded 2 documents\n",
      "Splitting 2 documents into chunks...\n",
      "splitted 2 documents Successfully! \n",
      "Loaded 1205 chunks from 2 document hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf\n",
      "Chunk list filled\n",
      "Total chunks so far: 7558\n"
     ]
    }
   ],
   "source": [
    "#  Store all 3 PDFs in vector database - Pinecone\n",
    "pdfs = [\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-1.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-2.pdf',\n",
    "    'hayatal-qulub-pdfs/Hayat-al-Qulub-Vol-3.pdf'\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for i, pdf in enumerate(pdfs):\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {i} documents\")\n",
    "    \n",
    "    # split text into chunks\n",
    "    print(f\"Splitting {i} documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"splitted {i} documents Successfully! \")\n",
    "    print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"Chunk list filled\")\n",
    "    print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in vector database - Pinecone\n",
    "vector_db = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e534d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_store = vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8115a6b",
   "metadata": {},
   "source": [
    "### **Vectorize multiple PDFs - functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_into_chunks(pdfs):\n",
    "    pdfs = list(pdfs)\n",
    "    all_chunks = []\n",
    "    for i, pdf in enumerate(pdfs):\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {i} documents\")\n",
    "        \n",
    "        # split text into chunks\n",
    "        print(f\"Splitting {i} documents into chunks...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"splitted {i} documents Successfully! \")\n",
    "        print(f\"Loaded {len(chunks)} chunks from {i} document {pdf}\")\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Chunk list filled\")\n",
    "        print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def store_chunks_in_pinecone(all_chunks, embedding_model, INDEX_NAME):\n",
    "    # create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model, model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "    print(f\"Created embeddings using model: {embedding_model}\")\n",
    "    \n",
    "    # Store in vector database - Pinecone\n",
    "    pinecone_store = PineconeVectorStore.from_documents(documents=all_chunks, embedding=embeddings, index_name=INDEX_NAME)\n",
    "    print(f\"Stored {len(all_chunks)} chunks in Pinecone index: {INDEX_NAME}\")\n",
    "    return pinecone_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2c22",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87829124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "pinecone_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings)\n",
    "\n",
    "# Create retriever from Pinecone\n",
    "retrieve = pinecone_store.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an Islamic history assistant. \n",
    "Always answer in a respectful and storytelling way. \n",
    "You have to understand the context deeply and answer accordingly.\n",
    "If the answer is not in the documents, say \"I don't know based on my knowledge.\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name='gpt-4o', \n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv('OPEN_AI_KEY')\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "input = {\"context\": retrieve | format_docs, \n",
    "      \"question\": RunnablePassthrough()\n",
    "      }\n",
    "\n",
    "\n",
    "qa_chain = ( input| prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab547dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the tapestry of Islamic history, the naming of Adam is a tale woven with profound meaning.\n",
      "\n",
      "It is narrated through authentic chains that Imam Muhammad al-Baqir and Ja‘far as-Sadiq (peace be upon them) said, \"Adam was named ‘Adam’ because he was ‘Adeemul Arz’, that is he was created from the face of the earth (from dust).\"\n",
      "\n",
      "So, Adam's name is intrinsically linked to his origin, a reminder of the humble earth from which he was formed.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Why Adam was named Adam? Give reference to it as well\"\n",
    "res = qa_chain.invoke(query)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadac881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aapka sawaal Hayat-ul-Qulub ke baare mein hai. Chaliye, main aapko iske baare mein tafseel se batata hoon:\n",
      "\n",
      "**Hayat-ul-Qulub kisne likhi hai?**\n",
      "\n",
      "Hayat-ul-Qulub Allama Muhammad Baqir Majlisi (Rehmatullah Alaih) ne likhi hai. Allama Majlisi ek mashhoor Islamic scholar aur Muhaddith the.\n",
      "\n",
      "**Ye kitab kiske baare mein hai?**\n",
      "\n",
      "Ye kitab Ambiya (Prophets), Aimma (Imams), aur deegar buzurg hastiyon ke ahwaal (life events) aur fazail (virtues) par mabni hai. Isme unke zindagi ke waqiyat, unke akhlaq, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "**Is kitab ki kitni jildhein (volumes) hain?**\n",
      "\n",
      "Hayat-ul-Qulub ki teen jildhein hain. Har jildh mein mukhtalif anbiya aur aimma ke baare mein tafseeli malumaat hain.\n",
      "\n",
      "**Har jildh ka mukhtasar khulasa (brief summary):**\n",
      "\n",
      "*   **Jildh 1:** Is jildh mein Ambiya-e-Kiram (Prophets) ke ahwaal bayan kiye gaye hain, jaise Hazrat Adam (A.S.), Hazrat Nuh (A.S.), Hazrat Ibrahim (A.S.), Hazrat Musa (A.S.), aur Hazrat Isa (A.S.). Har Nabi ki zindagi ke aham waqiyat aur unki ummat ke liye unke paighamat ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 2:** Is jildh mein Khatim-ul-Anbiya, Hazrat Muhammad Mustafa (S.A.W.) ki zindagi ke ahwaal bayan kiye gaye hain. Aapki wiladat se lekar aapki wafaat tak ke tamam aham waqiyat, aapke akhlaq, aapke mojizaat, aur aapki ummat ke liye aapki nasihaton ko tafseel se bayan kiya gaya hai.\n",
      "*   **Jildh 3:** Is jildh mein Aimma-e-Ahle Bait (A.S.) ke ahwaal bayan kiye gaye hain, jaise Imam Ali (A.S.), Imam Hasan (A.S.), Imam Hussain (A.S.), aur baqi tamam Aimma (A.S.). Har Imam ki zindagi ke aham waqiyat, unke fazail, aur unke maqamat ko tafseel se bayan kiya gaya hai.\n",
      "\n",
      "Hayat-ul-Qulub ek behtareen kitab hai jo Ambiya aur Aimma ke baare mein malumaat haasil karne ke liye nihayat mufeed hai. Is kitab ko padhne se insaan ke imaan mein izafa hota hai aur usko apni zindagi ko behtar tareeqe se guzarne ki hidayat milti hai.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"\"\"\n",
    "Kisne likh hai Hayatal Qulub? kiske bare me hai ye kitab? \n",
    "kitni jildhe hai iske aur kya har ek jildh ko short me smjha sakte ho?\n",
    "\"\"\"\n",
    "res2 = qa_chain.invoke(query1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81957866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check for any embedding wrapper you plan to use\n",
    "vec = embeddings.embed_query(\"test\")\n",
    "print(\"type:\", type(vec))\n",
    "print(\"len:\", len(vec))   # MUST be 1024 to match your Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52496e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "391fab36",
   "metadata": {},
   "source": [
    "## **Sulaym ibne Qays RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f0de4",
   "metadata": {},
   "source": [
    "### **RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b595684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ\")\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccabc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_into_chunks(pdfs: list):\n",
    "    print(f'Count of Documents - {len(pdfs)}')\n",
    "\n",
    "    all_chunks = []\n",
    "    for i, pdf in enumerate(pdfs):\n",
    "        i=+1\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {i} document successfully\")\n",
    "\n",
    "        # split into chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"Splitted {i} documment successfully!\")\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Total chunks so far: {len(all_chunks)}\")\n",
    "\n",
    "    return all_chunks\n",
    "    \n",
    "def store_chunks_in_pinecone(all_chunks, embedding_model, INDEX_NAME):\n",
    "     #create embeddings\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name = 'BAAI/bge-large-en-v1.5', model_kwargs={\"device\": \"cuda\"} \n",
    "        )\n",
    "    print(f\"Created embeddings using model: {embedding_model}\")\n",
    "\n",
    "    # Store in vector database - Pinecone\n",
    "    pinecone_store = PineconeVectorStore.from_documents(\n",
    "        documents=all_chunks,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    print(f\"Stored {len(all_chunks)} chunks in Pinecone index: {INDEX_NAME}\")\n",
    "    return pinecone_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc478a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Documents - 1\n",
      "Loaded 1 document successfully\n",
      "Splitted 1 documment successfully!\n",
      "Total chunks so far: 753\n"
     ]
    }
   ],
   "source": [
    "chunks = process_pdf_into_chunks(pdfs=['kitab_sulaim_ibn_qays_al-hilaali.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90bc1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings using model: BAAI/bge-large-en-v1.5\n",
      "Stored 753 chunks in Pinecone index: sulaym-ibne-qays\n"
     ]
    }
   ],
   "source": [
    "vector_store = store_chunks_in_pinecone(chunks, 'BAAI/bge-large-en-v1.5', INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add8e57",
   "metadata": {},
   "source": [
    "### **Retrieve QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faa8ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from system_prompt import system_prompt_text\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ\")\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "pinecone_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME, \n",
    "    embedding=embeddings)\n",
    "\n",
    "# Create retriever from Pinecone\n",
    "retrieve = pinecone_store.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = system_prompt_text\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# LLM and QA\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name='gpt-4o', \n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv('OPEN_AI_KEY')\n",
    "# )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "input = {\"context\": retrieve | format_docs, \n",
    "      \"question\": RunnablePassthrough()\n",
    "      }\n",
    "\n",
    "\n",
    "qa_chain = ( input| prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e4d0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events surrounding the attack on the house of Lady Fatimah (SA) are recounted in detail within the narrations of Kitab Sulaym ibn Qays. These accounts paint a vivid and sorrowful picture of the immediate aftermath of the Prophet's (PBUH) passing and the challenges faced by Imam Ali (AS) and his family.\n",
      "\n",
      "According to this tradition, after the Prophet's (PBUH) death, some individuals sought to secure allegiance for Abu Bakr. Imam Ali (AS), along with some of his loyal companions, remained in the house of Lady Fatimah (SA), mourning the loss of the Prophet (PBUH) and discussing the rightful succession.\n",
      "\n",
      "Sulaym ibn Qays narrates that Umar, along with a group of people, went to the house of Imam Ali (AS) seeking to compel him to pledge allegiance to Abu Bakr. When they arrived, Lady Fatimah (SA) was behind the door. Umar demanded that Imam Ali (AS) come out and pledge allegiance, threatening to burn the house down if he refused.\n",
      "\n",
      "Lady Fatimah (SA), standing behind the door, questioned Umar's audacity and reminded him of Allah's (SWT) commands. She asked, \"O Umar, do you not fear Allah Azz Wa Jall? You are entering my house and are surrounding my building.\"\n",
      "\n",
      "Despite her pleas, Umar persisted. He ordered his companions to gather wood and set it ablaze around the door of the house. In one account preserved in this book, it is mentioned that Umar himself participated in gathering the wood.\n",
      "\n",
      "The narrations describe a confrontation at the door, where Lady Fatimah (SA) was physically harmed. In one version, it is said that Umar struck her on the side with his sword, while in another, he struck her hand with a whip. Lady Fatimah (SA) cried out, \"O Father, O Messenger of Allah!\"\n",
      "\n",
      "Imam Ali (AS), hearing the commotion and his wife's distress, rushed to the door. He confronted Umar, grabbing him by the neck and throwing him to the ground. However, remembering the Prophet's (PBUH) instructions regarding patience and obedience, he restrained himself from further violence.\n",
      "\n",
      "The attackers then forced their way into the house, dragging Imam Ali (AS) to Abu Bakr to demand his allegiance. Salman al-Farsi, Abu Dharr al-Ghifari, Miqdad ibn al-Aswad, and other loyal companions of Imam Ali (AS) attempted to intervene, but they were outnumbered and overpowered.\n",
      "\n",
      "Salman recounts that Lady Fatimah (SA) was deeply distressed by these events. She cried out, \"O Father, O Prophet of Allah, Abu Bakr and Umar behaved so badly after you, while your eyes have not even closed in the grave.\"\n",
      "\n",
      "These events had a profound impact on Lady Fatimah (SA), contributing to her illness and eventual passing. The narrations in Kitab Sulaym ibn Qays serve as a testament to the suffering endured by the Ahlulbayt (AS) in the aftermath of the Prophet's (PBUH) death and highlight the injustices they faced.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Few people attacked the house of Lady Fatimah (SA) and burned the door. what was the entire incident? Explain in details.\"\n",
    "res = qa_chain.invoke(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea318175",
   "metadata": {},
   "source": [
    "## **LangChain Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628e6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadiq\\OneDrive\\Documents\\projects\\rag-for-Hayatal-Qulub\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5ba237",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QdrantClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# qdrant setup\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m qdrant_client = \u001b[43mQdrantClient\u001b[49m(\n\u001b[32m      3\u001b[39m     host=\u001b[33m\"\u001b[39m\u001b[33mxyz-example.eu-central.aws.cloud.qdrant.io\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mQDRANT_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'QdrantClient' is not defined"
     ]
    }
   ],
   "source": [
    "# qdrant setup\n",
    "qdrant_client = QdrantClient(\n",
    "    host=\"xyz-example.eu-central.aws.cloud.qdrant.io\",\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65138dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- begin of pipelines ----------\n",
    "# load pdf\n",
    "pdf_path = 'Hayat-Qulub-Alama-Majlisi.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cded5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L12-v2\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3b560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vectordb and store embeddings\n",
    "vector_db_qdrant = qdrant_client.recreate_collection(\n",
    "    collection_name=\"hayat_qulub\",\n",
    "    vectors_config=Qdrant.VectorParams(\n",
    "         size = 768,\n",
    "        distance=Qdrant.Distance.COSINE\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de4611",
   "metadata": {},
   "source": [
    "## **LlamaIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader \n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader('pdf_docs').load_data()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e58299",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbedding(model_name=\"all-MiniLM-L12-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(index_store=embeddings)\n",
    "storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2c1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
