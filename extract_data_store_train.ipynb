{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9cd1d0",
   "metadata": {},
   "source": [
    "## **Extract Hadith Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2afd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_hadiths_from_pdf(pdf_path):\n",
    "    \"\"\"Extract hadiths using PyMuPDF (more reliable)\"\"\"\n",
    "    \n",
    "    print(f\"üìñ Opening PDF with PyMuPDF: {pdf_path}\")\n",
    "    \n",
    "    # Open PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    print(f\"üìÑ Total pages: {total_pages}\")\n",
    "    \n",
    "    # Extract all text with page tracking\n",
    "    all_text = \"\"\n",
    "    page_map = {}\n",
    "    current_pos = 0\n",
    "    \n",
    "    for page_num in range(total_pages):\n",
    "        page = doc[page_num]\n",
    "        page_text = page.get_text()\n",
    "        \n",
    "        page_start = current_pos\n",
    "        page_end = current_pos + len(page_text)\n",
    "        \n",
    "        # Map character positions to page numbers\n",
    "        for pos in range(page_start, page_end):\n",
    "            page_map[pos] = page_num + 1  # 1-indexed\n",
    "        \n",
    "        all_text += page_text + \"\\n\"\n",
    "        current_pos = page_end + 1\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(all_text)} characters from PDF\")\n",
    "    \n",
    "    # Show a sample of the text\n",
    "    print(\"\\nüìù Sample of extracted text (first 500 chars):\")\n",
    "    print(all_text[:500])\n",
    "    print(\"...\")\n",
    "    \n",
    "    # Try multiple patterns\n",
    "    print(\"\\nüîç Searching for Hadith patterns...\")\n",
    "    \n",
    "    patterns = [\n",
    "        r'Hadith\\s+No\\.\\s*(\\d+)',              # Standard: Hadith No. 3\n",
    "        r'Hadƒ´th\\s+No\\.\\s*(\\d+)',              # With macron\n",
    "        r'Hadith\\s+No\\s+(\\d+)',                # Without period\n",
    "        r'(?i)hadith\\s*no\\.?\\s*(\\d+)',        # Case insensitive\n",
    "        r'Hadƒ´th\\s+No\\s+(\\d+)',                # Macron without period\n",
    "    ]\n",
    "    \n",
    "    best_matches = []\n",
    "    best_pattern = None\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = list(re.finditer(pattern, all_text))\n",
    "        print(f\"  Pattern '{pattern}': Found {len(matches)} matches\")\n",
    "        if len(matches) > len(best_matches):\n",
    "            best_matches = matches\n",
    "            best_pattern = pattern\n",
    "    \n",
    "    if len(best_matches) == 0:\n",
    "        print(\"\\n‚ùå No hadith headers found!\")\n",
    "        print(\"\\nüí° Let's search for 'Hadith' (case insensitive):\")\n",
    "        sample_matches = re.finditer(r'(?i)hadith', all_text)\n",
    "        for i, m in enumerate(list(sample_matches)[:10]):\n",
    "            start = max(0, m.start() - 20)\n",
    "            end = min(len(all_text), m.end() + 50)\n",
    "            print(f\"  Match {i+1}: ...{all_text[start:end]}...\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n‚úÖ Using pattern: '{best_pattern}' ({len(best_matches)} matches)\")\n",
    "    \n",
    "    # Extract hadiths\n",
    "    hadiths = []\n",
    "    \n",
    "    for i, match in enumerate(best_matches):\n",
    "        hadith_num = match.group(1)\n",
    "        start_pos = match.end()\n",
    "        \n",
    "        # Find where this hadith ends\n",
    "        if i < len(best_matches) - 1:\n",
    "            end_pos = best_matches[i + 1].start()\n",
    "        else:\n",
    "            end_pos = len(all_text)\n",
    "        \n",
    "        # Extract text\n",
    "        hadith_text = all_text[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Skip if too short\n",
    "        if len(hadith_text) < 50:\n",
    "            print(f\"  ‚ö†Ô∏è Skipping Hadith {hadith_num}: Too short\")\n",
    "            continue\n",
    "        \n",
    "        # Get page range\n",
    "        start_page = page_map.get(match.start(), 'Unknown')\n",
    "        end_page = page_map.get(end_pos - 1, start_page)\n",
    "        \n",
    "        page_range = f\"{start_page}\" if start_page == end_page else f\"{start_page}-{end_page}\"\n",
    "        \n",
    "        hadith_data = {\n",
    "            'hadith_no': int(hadith_num),\n",
    "            'hadith_text': hadith_text,\n",
    "            'page_start': start_page,\n",
    "            'page_end': end_page,\n",
    "            'page_range': page_range,\n",
    "            'char_count': len(hadith_text),\n",
    "            'word_count': len(hadith_text.split()),\n",
    "            'extracted_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        hadiths.append(hadith_data)\n",
    "        print(f\"  ‚úì Hadith {hadith_num}: Pages {page_range}, {hadith_data['word_count']} words\")\n",
    "    \n",
    "    return hadiths\n",
    "\n",
    "# Rest of the save functions remain the same...\n",
    "\n",
    "def save_to_json(hadiths, output_path='hadiths.json'):\n",
    "    \"\"\"Save hadiths to JSON file\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(hadiths, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n‚úÖ Saved {len(hadiths)} hadiths to {output_path}\")\n",
    "\n",
    "def save_to_csv(hadiths, output_path='hadiths.csv'):\n",
    "    \"\"\"Save hadiths to CSV file\"\"\"\n",
    "    df = pd.DataFrame(hadiths)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Saved {len(hadiths)} hadiths to {output_path}\")\n",
    "\n",
    "# def save_to_excel(hadiths, output_path='hadiths.xlsx'):\n",
    "#     \"\"\"Save hadiths to Excel file\"\"\"\n",
    "#     df = pd.DataFrame(hadiths)\n",
    "#     df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "#     print(f\"‚úÖ Saved {len(hadiths)} hadiths to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9ad619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ EXTRACTING HADITHS FROM KITAB SULAYM IBN QAYS\n",
      "======================================================================\n",
      "\n",
      "üìñ Opening PDF with PyMuPDF: kitab_sulaim_ibn_qays_al-hilaali.pdf\n",
      "üìÑ Total pages: 255\n",
      "‚úÖ Extracted 562017 characters from PDF\n",
      "\n",
      "üìù Sample of extracted text (first 500 chars):\n",
      " \n",
      " \n",
      "KITAB-E-SULAYM \n",
      "IBN \n",
      "QAYS AL HILALI \n",
      " \n",
      "Imam Ja‚Äôfar Al Sadiq (a.s) said: \n",
      "‚ÄúIf anyone from our Shia and devotees does not have the book of Sulaym \n",
      "ibn Qays al Hilali, then he does not have any of our things, and he does not \n",
      "know any of our matters.  \n",
      "This is the first book of Shia and is one of the secrets of Ale-Muhammad \n",
      "(a.s)‚Äù \n",
      " \n",
      "Introduction \n",
      " \n",
      "BISMILLAHIR RAHMANIR RAHEEM \n",
      " \n",
      "Wa Sallallahu ‚ÄòAla Muhammad Wa Alahit Tayyabin al Muntakhabin \n",
      " \n",
      "And May Allah shower his blessings on the Holy Pro\n",
      "...\n",
      "\n",
      "üîç Searching for Hadith patterns...\n",
      "  Pattern 'Hadith\\s+No\\.\\s*(\\d+)': Found 66 matches\n",
      "  Pattern 'Hadƒ´th\\s+No\\.\\s*(\\d+)': Found 0 matches\n",
      "  Pattern 'Hadith\\s+No\\s+(\\d+)': Found 0 matches\n",
      "  Pattern '(?i)hadith\\s*no\\.?\\s*(\\d+)': Found 91 matches\n",
      "  Pattern 'Hadƒ´th\\s+No\\s+(\\d+)': Found 0 matches\n",
      "\n",
      "‚úÖ Using pattern: '(?i)hadith\\s*no\\.?\\s*(\\d+)' (91 matches)\n",
      "  ‚úì Hadith 1: Pages 6-9, 1182 words\n",
      "  ‚úì Hadith 2: Pages 9-11, 657 words\n",
      "  ‚úì Hadith 3: Pages 11-15, 1705 words\n",
      "  ‚úì Hadith 4: Pages 15-30, 6461 words\n",
      "  ‚úì Hadith 5: Pages 30-31, 147 words\n",
      "  ‚úì Hadith 6: Pages 31-33, 966 words\n",
      "  ‚úì Hadith 7: Pages 33-37, 1986 words\n",
      "  ‚úì Hadith 8: Pages 37-41, 1222 words\n",
      "  ‚úì Hadith 9: Pages 41-42, 568 words\n",
      "  ‚úì Hadith 10: Pages 42-49, 3150 words\n",
      "  ‚úì Hadith 11: Pages 49-66, 6988 words\n",
      "  ‚úì Hadith 12: Pages 66-73, 3228 words\n",
      "  ‚úì Hadith 1: Pages 73-75, 616 words\n",
      "  ‚úì Hadith 14: Pages 75-88, 5623 words\n",
      "  ‚úì Hadith 15: Pages 88-93, 2360 words\n",
      "  ‚úì Hadith 16: Pages 93-98, 1860 words\n",
      "  ‚úì Hadith 17: Pages 98-103, 2186 words\n",
      "  ‚úì Hadith 18: Pages 103-105, 1053 words\n",
      "  ‚úì Hadith 19: Pages 105-107, 1057 words\n",
      "  ‚úì Hadith 20: Pages 107-109, 818 words\n",
      "  ‚úì Hadith 21: Pages 109-112, 1000 words\n",
      "  ‚úì Hadith 22: Pages 112-114, 854 words\n",
      "  ‚úì Hadith 23: Pages 114-120, 1902 words\n",
      "  ‚úì Hadith 24: Pages 120, 219 words\n",
      "  ‚úì Hadith 25: Pages 120-146, 9172 words\n",
      "  ‚úì Hadith 26: Pages 146-157, 4644 words\n",
      "  ‚úì Hadith 27: Pages 157-158, 287 words\n",
      "  ‚úì Hadith 28: Pages 158-159, 307 words\n",
      "  ‚úì Hadith 29: Pages 159-161, 823 words\n",
      "  ‚úì Hadith 30: Pages 161, 184 words\n",
      "  ‚úì Hadith 31: Pages 161-162, 135 words\n",
      "  ‚úì Hadith 32: Pages 162, 222 words\n",
      "  ‚úì Hadith 33: Pages 162-163, 121 words\n",
      "  ‚úì Hadith 34: Pages 163-166, 1725 words\n",
      "  ‚úì Hadith 35: Pages 166-167, 435 words\n",
      "  ‚úì Hadith 36: Pages 167-169, 545 words\n",
      "  ‚úì Hadith 37: Pages 169-173, 1996 words\n",
      "  ‚úì Hadith 38: Pages 173-174, 530 words\n",
      "  ‚úì Hadith 39: Pages 174-176, 687 words\n",
      "  ‚úì Hadith 40: Pages 176-177, 364 words\n",
      "  ‚úì Hadith 41: Pages 177-178, 368 words\n",
      "  ‚úì Hadith 42: Pages 178-186, 3552 words\n",
      "  ‚úì Hadith 43: Pages 186-191, 1891 words\n",
      "  ‚úì Hadith 44: Pages 191-194, 883 words\n",
      "  ‚úì Hadith 45: Pages 194-195, 619 words\n",
      "  ‚úì Hadith 46: Pages 195-197, 803 words\n",
      "  ‚úì Hadith 47: Pages 197-198, 37 words\n",
      "  ‚úì Hadith 48: Pages 198-206, 3513 words\n",
      "  ‚úì Hadith 49: Pages 206-207, 465 words\n",
      "  ‚úì Hadith 50: Pages 207-208, 179 words\n",
      "  ‚úì Hadith 51: Pages 208, 92 words\n",
      "  ‚úì Hadith 52: Pages 208-209, 258 words\n",
      "  ‚úì Hadith 53: Pages 209, 154 words\n",
      "  ‚úì Hadith 54: Pages 209-212, 966 words\n",
      "  ‚úì Hadith 55: Pages 212-213, 701 words\n",
      "  ‚úì Hadith 56: Pages 213-214, 98 words\n",
      "  ‚úì Hadith 57: Pages 214, 151 words\n",
      "  ‚úì Hadith 58: Pages 214-220, 2578 words\n",
      "  ‚úì Hadith 59: Pages 220-221, 101 words\n",
      "  ‚úì Hadith 60: Pages 221-222, 610 words\n",
      "  ‚úì Hadith 61: Pages 222-226, 1465 words\n",
      "  ‚úì Hadith 62: Pages 226-227, 673 words\n",
      "  ‚úì Hadith 63: Pages 227-228, 124 words\n",
      "  ‚úì Hadith 64: Pages 228, 45 words\n",
      "  ‚úì Hadith 65: Pages 228-229, 302 words\n",
      "  ‚úì Hadith 66: Pages 229-230, 640 words\n",
      "  ‚úì Hadith 67: Pages 230-236, 2493 words\n",
      "  ‚úì Hadith 68: Pages 236, 165 words\n",
      "  ‚úì Hadith 69: Pages 236-239, 1026 words\n",
      "  ‚úì Hadith 70: Pages 239-241, 536 words\n",
      "  ‚úì Hadith 71: Pages 241, 233 words\n",
      "  ‚úì Hadith 72: Pages 241-242, 494 words\n",
      "  ‚úì Hadith 73: Pages 242-243, 254 words\n",
      "  ‚úì Hadith 74: Pages 243, 94 words\n",
      "  ‚úì Hadith 75: Pages 243-244, 227 words\n",
      "  ‚úì Hadith 76: Pages 244-245, 372 words\n",
      "  ‚úì Hadith 77: Pages 245, 176 words\n",
      "  ‚úì Hadith 78: Pages 245-247, 535 words\n",
      "  ‚úì Hadith 79: Pages 247, 214 words\n",
      "  ‚úì Hadith 80: Pages 247-248, 176 words\n",
      "  ‚úì Hadith 81: Pages 248, 86 words\n",
      "  ‚úì Hadith 82: Pages 248, 90 words\n",
      "  ‚úì Hadith 83: Pages 248-249, 95 words\n",
      "  ‚úì Hadith 84: Pages 249, 116 words\n",
      "  ‚úì Hadith 85: Pages 249, 148 words\n",
      "  ‚úì Hadith 86: Pages 249-252, 1064 words\n",
      "  ‚úì Hadith 87: Pages 252-253, 138 words\n",
      "  ‚úì Hadith 88: Pages 253-254, 251 words\n",
      "  ‚úì Hadith 89: Pages 254, 228 words\n",
      "  ‚úì Hadith 90: Pages 254-255, 114 words\n",
      "  ‚úì Hadith 91: Pages 255, 188 words\n",
      "\n",
      "‚úÖ Saved 91 hadiths to hadiths.json\n",
      "‚úÖ Saved 91 hadiths to hadiths.csv\n",
      "\n",
      "======================================================================\n",
      "üìä EXTRACTION STATISTICS\n",
      "======================================================================\n",
      "Total Hadiths: 91\n",
      "Shortest Hadith: 37 words\n",
      "Longest Hadith: 9172 words\n",
      "Average Length: 1107 words\n",
      "\n",
      "üìñ Sample Hadiths:\n",
      "\n",
      "  Hadith No. 1 (Page 6-9)\n",
      "  Preview: Sulaym says: ‚ÄúI heard Salman al-Farsi saying: ‚ÄúI was sitting with the Holy \n",
      "Prophet (SAW) while he was in that period of illness in which he passed \n",
      "a...\n",
      "\n",
      "  Hadith No. 2 (Page 9-11)\n",
      "  Preview: Sulaym said: ‚ÄúI was told by Hadhrat Ali ibn Abu Talib (AS). He said: ‚ÄúI was \n",
      "going somewhere in Medina with the Holy Prophet (SAW). We came \n",
      "towards a...\n",
      "\n",
      "  Hadith No. 3 (Page 11-15)\n",
      "  Preview: Sulaym narrated: He said: ‚ÄúI heard Al Bara ibn Adhib saying: ‚ÄúI loved Bani \n",
      "Hashem very much, both during the life time and after the death of the Hol...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EXTRACTION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ EXTRACTING HADITHS FROM KITAB SULAYM IBN QAYS\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Extract hadiths\n",
    "    hadiths = extract_hadiths_from_pdf('kitab_sulaim_ibn_qays_al-hilaali.pdf')\n",
    "    \n",
    "    # Save in multiple formats\n",
    "    save_to_json(hadiths)\n",
    "    save_to_csv(hadiths)\n",
    "    # save_to_excel(hadiths)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä EXTRACTION STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total Hadiths: {len(hadiths)}\")\n",
    "    print(f\"Shortest Hadith: {min(h['word_count'] for h in hadiths)} words\")\n",
    "    print(f\"Longest Hadith: {max(h['word_count'] for h in hadiths)} words\")\n",
    "    print(f\"Average Length: {sum(h['word_count'] for h in hadiths) // len(hadiths)} words\")\n",
    "    \n",
    "    # Show first 3 hadiths as sample\n",
    "    print(\"\\nüìñ Sample Hadiths:\")\n",
    "    for hadith in hadiths[:3]:\n",
    "        print(f\"\\n  Hadith No. {hadith['hadith_no']} (Page {hadith['page_range']})\")\n",
    "        print(f\"  Preview: {hadith['hadith_text'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8513b52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîß UPDATING HADITHS.JSON\n",
      "======================================================================\n",
      "\n",
      "üìñ Reading hadiths.json...\n",
      "‚úÖ Loaded 92 entries\n",
      "‚úÖ Updated 92 entries\n",
      "‚úÖ Saved to hadiths_updated.json\n",
      "\n",
      "üìä Summary:\n",
      "  Preface entries: 1\n",
      "  Hadith entries: 91\n",
      "  Total: 92\n",
      "\n",
      "üìÑ Sample entries:\n",
      "\n",
      "  Entry 1:\n",
      "    Hadith No: 0\n",
      "    Section Type: preface\n",
      "    Page Range: N/A\n",
      "    Preview: Imam Ja‚Äôfar Al Sadiq (a.s) said: ‚ÄúIf anyone from our Shia and devotees does not ...\n",
      "\n",
      "  Entry 2:\n",
      "    Hadith No: 1\n",
      "    Section Type: hadith\n",
      "    Page Range: 6-9\n",
      "    Preview: Sulaym says: ‚ÄúI heard Salman al-Farsi saying: ‚ÄúI was sitting with the Holy \n",
      "Prop...\n",
      "\n",
      "  Entry 3:\n",
      "    Hadith No: 2\n",
      "    Section Type: hadith\n",
      "    Page Range: 9-11\n",
      "    Preview: Sulaym said: ‚ÄúI was told by Hadhrat Ali ibn Abu Talib (AS). He said: ‚ÄúI was \n",
      "goi...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ UPDATE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üí° Your updated file is: hadiths_updated.json\n",
      "üí° To replace original: rename hadiths_updated.json ‚Üí hadiths.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def update_hadiths_json(input_file='hadiths.json', output_file='hadiths_updated.json'):\n",
    "    \"\"\"Add section_type to existing hadiths.json and remove is_preface\"\"\"\n",
    "    \n",
    "    print(f\"üìñ Reading {input_file}...\")\n",
    "    \n",
    "    # Load existing JSON\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        hadiths = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(hadiths)} entries\")\n",
    "    \n",
    "    # Update each entry\n",
    "    updated_count = 0\n",
    "    for hadith in hadiths:\n",
    "        # Determine section_type based on hadith_no\n",
    "        if hadith['hadith_no'] == 0:\n",
    "            hadith['section_type'] = 'preface'\n",
    "        else:\n",
    "            hadith['section_type'] = 'hadith'\n",
    "        \n",
    "        # Remove is_preface key if it exists\n",
    "        if 'is_preface' in hadith:\n",
    "            del hadith['is_preface']\n",
    "        \n",
    "        updated_count += 1\n",
    "    \n",
    "    print(f\"‚úÖ Updated {updated_count} entries\")\n",
    "    \n",
    "    # Save updated JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(hadiths, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved to {output_file}\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    preface_count = sum(1 for h in hadiths if h['section_type'] == 'preface')\n",
    "    hadith_count = sum(1 for h in hadiths if h['section_type'] == 'hadith')\n",
    "    print(f\"  Preface entries: {preface_count}\")\n",
    "    print(f\"  Hadith entries: {hadith_count}\")\n",
    "    print(f\"  Total: {len(hadiths)}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nüìÑ Sample entries:\")\n",
    "    for i, hadith in enumerate(hadiths[:3]):\n",
    "        print(f\"\\n  Entry {i+1}:\")\n",
    "        print(f\"    Hadith No: {hadith['hadith_no']}\")\n",
    "        print(f\"    Section Type: {hadith['section_type']}\")\n",
    "        print(f\"    Page Range: {hadith.get('page_range', 'N/A')}\")\n",
    "        print(f\"    Preview: {hadith['hadith_text'][:80]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîß UPDATING HADITHS.JSON\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    update_hadiths_json()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ UPDATE COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüí° Your updated file is: hadiths_updated.json\")\n",
    "    print(\"üí° To replace original: rename hadiths_updated.json ‚Üí hadiths.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e23f3f",
   "metadata": {},
   "source": [
    "## **Train the JSON Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0406639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ TRAINING RAG WITH STRUCTURED HADITHS\n",
      "======================================================================\n",
      "\n",
      "üìñ Loaded 92 hadiths from hadiths_updated.json\n",
      "‚úÖ Created 92 documents\n",
      "\n",
      "üìÑ Sample Document:\n",
      "Hadith No: 1\n",
      "Section Type: hadith\n",
      "Page Range: 6-9\n",
      "Text Preview: Sulaym says: ‚ÄúI heard Salman al-Farsi saying: ‚ÄúI was sitting with the Holy \n",
      "Prophet (SAW) while he was in that period of illness in which he passed \n",
      "a...\n",
      "\n",
      "üìä Metadata size: 199 bytes (limit: 40960 bytes)\n",
      "‚úÖ Metadata size OK\n",
      "\n",
      "======================================================================\n",
      "üîÑ Creating embeddings and storing in Pinecone...\n",
      "   This may take a few minutes for 92 documents...\n"
     ]
    },
    {
     "ename": "PineconeApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Sun, 26 Oct 2025 18:15:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '115', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1778', 'x-pinecone-request-id': '1677343850328051571', 'x-envoy-upstream-service-time': '73', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Metadata size is 49917 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPineconeApiException\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Store in Pinecone\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m vector_store = \u001b[43mstore_in_pinecone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINDEX_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Test retrieval\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Testing retrieval...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mstore_in_pinecone\u001b[39m\u001b[34m(documents, INDEX_NAME)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   This may take a few minutes for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Store in batches to avoid timeouts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m pinecone_store = \u001b[43mPineconeVectorStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINDEX_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ‚Üê Process in smaller batches\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m hadiths in Pinecone index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINDEX_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pinecone_store\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:837\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    835\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:926\u001b[39m, in \u001b[36mPineconeVectorStore.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m pinecone_index = \u001b[38;5;28mcls\u001b[39m.get_pinecone_index(index_name, pool_threads)\n\u001b[32m    924\u001b[39m pinecone = \u001b[38;5;28mcls\u001b[39m(pinecone_index, embedding, text_key, namespace, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[43mpinecone\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsert_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pinecone\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:363\u001b[39m, in \u001b[36mPineconeVectorStore.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[32m    354\u001b[39m     async_res = [\n\u001b[32m    355\u001b[39m         \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    356\u001b[39m             vectors=batch_vector_tuples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[32m    362\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_res\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    366\u001b[39m         vectors=vector_tuples,\n\u001b[32m    367\u001b[39m         namespace=namespace,\n\u001b[32m    368\u001b[39m         async_req=async_req,\n\u001b[32m    369\u001b[39m         **kwargs,\n\u001b[32m    370\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:363\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[32m    354\u001b[39m     async_res = [\n\u001b[32m    355\u001b[39m         \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    356\u001b[39m             vectors=batch_vector_tuples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[32m    362\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     [\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m async_res]\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    366\u001b[39m         vectors=vector_tuples,\n\u001b[32m    367\u001b[39m         namespace=namespace,\n\u001b[32m    368\u001b[39m         async_req=async_req,\n\u001b[32m    369\u001b[39m         **kwargs,\n\u001b[32m    370\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:182\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    184\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response_data\n\u001b[32m    186\u001b[39m return_data = response_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:170\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    161\u001b[39m url = build_request_url(\n\u001b[32m    162\u001b[39m     config=config,\n\u001b[32m    163\u001b[39m     processed_path_params=path_params_tuple,\n\u001b[32m    164\u001b[39m     resource_path=resource_path,\n\u001b[32m    165\u001b[39m     _host=_host,\n\u001b[32m    166\u001b[39m )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     response_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_query_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_post_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:386\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.OPTIONS(\n\u001b[32m    377\u001b[39m         url,\n\u001b[32m    378\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m         body=body,\n\u001b[32m    384\u001b[39m     )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPUT\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.PUT(\n\u001b[32m    397\u001b[39m         url,\n\u001b[32m    398\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m         body=body,\n\u001b[32m    404\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:146\u001b[39m, in \u001b[36mRestClientInterface.POST\u001b[39m\u001b[34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPOST\u001b[39m(\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    138\u001b[39m     url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m     _request_timeout=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    145\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py:267\u001b[39m, in \u001b[36mUrllib3RestClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# log response body\u001b[39;00m\n\u001b[32m    265\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, r.data)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mraise_exceptions_or_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Sadiq\\rag-for-hq-books\\venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:49\u001b[39m, in \u001b[36mraise_exceptions_or_return\u001b[39m\u001b[34m(r)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m500\u001b[39m <= r.status <= \u001b[32m599\u001b[39m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp=r)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp=r)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[31mPineconeApiException\u001b[39m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Sun, 26 Oct 2025 18:15:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '115', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1778', 'x-pinecone-request-id': '1677343850328051571', 'x-envoy-upstream-service-time': '73', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Metadata size is 49917 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ_V2\")\n",
    "\n",
    "def load_structured_hadiths(json_path='hadiths_updated.json'):\n",
    "    \"\"\"Load hadiths from JSON\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        hadiths = json.load(f)\n",
    "    print(f\"üìñ Loaded {len(hadiths)} hadiths from {json_path}\")\n",
    "    return hadiths\n",
    "\n",
    "def create_documents_from_hadiths(hadiths):\n",
    "    \"\"\"Convert hadiths to LangChain documents\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for hadith in hadiths:\n",
    "        # CRITICAL: Put full text in page_content (no size limit)\n",
    "        # Only put small reference data in metadata\n",
    "        doc = Document(\n",
    "            page_content=hadith['hadith_text'],  # ‚Üê Full text here (unlimited)\n",
    "            metadata={\n",
    "                'hadith_no': hadith['hadith_no'],\n",
    "                'section_type': hadith.get('section_type', 'hadith'),\n",
    "                'page_start': hadith.get('page_start', 'N/A'),\n",
    "                'page_end': hadith.get('page_end', 'N/A'),\n",
    "                'page_range': hadith.get('page_range', 'N/A'),\n",
    "                'word_count': hadith.get('word_count', 0),\n",
    "                'source': 'Kitab Sulaym ibn Qays'\n",
    "                # ‚Üê No hadith_text in metadata! Only references\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def store_in_pinecone(documents, INDEX_NAME):\n",
    "    \"\"\"Store documents in Pinecone\"\"\"\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='BAAI/bge-large-en-v1.5',\n",
    "        model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "    print(\"üîÑ Creating embeddings and storing in Pinecone...\")\n",
    "    print(f\"   This may take a few minutes for {len(documents)} documents...\")\n",
    "    \n",
    "    # Store in batches to avoid timeouts\n",
    "    pinecone_store = PineconeVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME,\n",
    "        batch_size=50  # ‚Üê Process in smaller batches\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Stored {len(documents)} hadiths in Pinecone index: {INDEX_NAME}\")\n",
    "    \n",
    "    return pinecone_store\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ TRAINING RAG WITH STRUCTURED HADITHS\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Load structured hadiths\n",
    "    hadiths = load_structured_hadiths()\n",
    "    \n",
    "    # Convert to documents\n",
    "    documents = create_documents_from_hadiths(hadiths)\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nüìÑ Sample Document:\")\n",
    "    print(f\"Hadith No: {documents[1].metadata['hadith_no']}\")\n",
    "    print(f\"Section Type: {documents[1].metadata['section_type']}\")\n",
    "    print(f\"Page Range: {documents[1].metadata['page_range']}\")\n",
    "    print(f\"Text Preview: {documents[1].page_content[:150]}...\")\n",
    "    \n",
    "    # Check metadata size\n",
    "    import sys\n",
    "    metadata_size = sys.getsizeof(str(documents[1].metadata))\n",
    "    print(f\"\\nüìä Metadata size: {metadata_size} bytes (limit: 40960 bytes)\")\n",
    "    \n",
    "    if metadata_size > 40000:\n",
    "        print(\"‚ö†Ô∏è WARNING: Metadata too large!\")\n",
    "    else:\n",
    "        print(\"‚úÖ Metadata size OK\")\n",
    "    \n",
    "    # Store in Pinecone\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    vector_store = store_in_pinecone(documents, INDEX_NAME)\n",
    "    \n",
    "    # Test retrieval\n",
    "    print(\"\\nüîç Testing retrieval...\")\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    test_results = retriever.get_relevant_documents(\"attack on Fatimah house\")\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(test_results)} hadiths:\")\n",
    "    for doc in test_results:\n",
    "        print(f\"\\n  Hadith No. {doc.metadata['hadith_no']} (Pages {doc.metadata['page_range']})\")\n",
    "        print(f\"  Section: {doc.metadata['section_type']}\")\n",
    "        print(f\"  Preview: {doc.page_content[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f096656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fac58934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest hadith: No. 25\n",
      "Text length: 49778 characters\n",
      "Text size: 99630 bytes\n",
      "\n",
      "Metadata without text:\n",
      "Size: 240 bytes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "# Load your JSON\n",
    "with open('hadiths_updated.json', 'r', encoding='utf-8') as f:\n",
    "    hadiths = json.load(f)\n",
    "\n",
    "# Find the largest hadith\n",
    "largest = max(hadiths, key=lambda h: len(h['hadith_text']))\n",
    "\n",
    "print(f\"Largest hadith: No. {largest['hadith_no']}\")\n",
    "print(f\"Text length: {len(largest['hadith_text'])} characters\")\n",
    "print(f\"Text size: {sys.getsizeof(largest['hadith_text'])} bytes\")\n",
    "print(f\"\\nMetadata without text:\")\n",
    "metadata = {k: v for k, v in largest.items() if k != 'hadith_text'}\n",
    "print(f\"Size: {sys.getsizeof(str(metadata))} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4a9666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STORING HADITHS IN PINECONE (MANUAL METHOD)\n",
      "======================================================================\n",
      "\n",
      "üìñ Loaded 92 hadiths\n",
      "\n",
      "üìä Checking metadata size for largest hadith...\n",
      "‚úÖ Metadata size: 172 bytes (limit: 40960)\n",
      "‚úÖ Safe to proceed!\n",
      "\n",
      "üîÑ Initializing embedding model...\n",
      "üîÑ Processing 92 hadiths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 49/92 [00:03<00:02, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92/92 [00:08<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded final batch of 42\n",
      "‚úÖ Stored 92 hadiths in Pinecone\n",
      "üìä Index stats: 246 vectors\n",
      "\n",
      "======================================================================\n",
      "‚úÖ STORAGE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üí° Next: Use the HadithRAG class for retrieval\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME_SQ_V2\")\n",
    "\n",
    "def load_hadiths(json_path='hadiths_updated.json'):\n",
    "    \"\"\"Load hadiths from JSON\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        hadiths = json.load(f)\n",
    "    print(f\"üìñ Loaded {len(hadiths)} hadiths\")\n",
    "    return hadiths\n",
    "\n",
    "def store_in_pinecone_manual(hadiths, INDEX_NAME):\n",
    "    \"\"\"Store hadiths directly in Pinecone (bypass LangChain)\"\"\"\n",
    "    \n",
    "    # Initialize embeddings model\n",
    "    print(\"üîÑ Initializing embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='BAAI/bge-large-en-v1.5',\n",
    "        model_kwargs={\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "    # Get Pinecone index\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(hadiths)} hadiths...\")\n",
    "    \n",
    "    vectors_to_upsert = []\n",
    "    \n",
    "    for hadith in tqdm(hadiths, desc=\"Creating embeddings\"):\n",
    "        # Create embedding for the text\n",
    "        text = hadith['hadith_text']\n",
    "        embedding = embeddings.embed_query(text)\n",
    "        \n",
    "        # Create MINIMAL metadata (NO TEXT!)\n",
    "        metadata = {\n",
    "            'hadith_no': int(hadith['hadith_no']),\n",
    "            'section_type': hadith.get('section_type', 'hadith'),\n",
    "            'page_range': hadith.get('page_range', 'N/A'),\n",
    "            'word_count': int(hadith.get('word_count', 0)),\n",
    "            'source': 'Kitab Sulaym ibn Qays'\n",
    "        }\n",
    "        \n",
    "        # Verify metadata size\n",
    "        metadata_size = sys.getsizeof(str(metadata))\n",
    "        if metadata_size > 40000:\n",
    "            print(f\"‚ö†Ô∏è Hadith {hadith['hadith_no']} metadata too large: {metadata_size} bytes\")\n",
    "            continue\n",
    "        \n",
    "        # Create vector ID\n",
    "        vector_id = f\"hadith_{hadith['hadith_no']}\"\n",
    "        \n",
    "        # Add to batch\n",
    "        vectors_to_upsert.append({\n",
    "            'id': vector_id,\n",
    "            'values': embedding,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "        \n",
    "        # Upsert in batches of 50\n",
    "        if len(vectors_to_upsert) >= 50:\n",
    "            try:\n",
    "                index.upsert(vectors=vectors_to_upsert)\n",
    "                print(f\"‚úÖ Uploaded batch of {len(vectors_to_upsert)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error uploading batch: {e}\")\n",
    "            vectors_to_upsert = []\n",
    "    \n",
    "    # Upsert remaining vectors\n",
    "    if vectors_to_upsert:\n",
    "        try:\n",
    "            index.upsert(vectors=vectors_to_upsert)\n",
    "            print(f\"‚úÖ Uploaded final batch of {len(vectors_to_upsert)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading final batch: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Stored {len(hadiths)} hadiths in Pinecone\")\n",
    "    \n",
    "    # Verify\n",
    "    stats = index.describe_index_stats()\n",
    "    print(f\"üìä Index stats: {stats['total_vector_count']} vectors\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ STORING HADITHS IN PINECONE (MANUAL METHOD)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Load hadiths\n",
    "    hadiths = load_hadiths()\n",
    "    \n",
    "    # Check metadata size for largest hadith\n",
    "    print(\"\\nüìä Checking metadata size for largest hadith...\")\n",
    "    largest = max(hadiths, key=lambda h: len(h['hadith_text']))\n",
    "    sample_metadata = {\n",
    "        'hadith_no': largest['hadith_no'],\n",
    "        'section_type': 'hadith',\n",
    "        'page_range': largest.get('page_range', 'N/A'),\n",
    "        'word_count': largest.get('word_count', 0),\n",
    "        'source': 'Kitab Sulaym ibn Qays'\n",
    "    }\n",
    "    size = sys.getsizeof(str(sample_metadata))\n",
    "    print(f\"‚úÖ Metadata size: {size} bytes (limit: 40960)\")\n",
    "    print(f\"‚úÖ Safe to proceed!\\n\")\n",
    "    \n",
    "    # Store in Pinecone\n",
    "    store_in_pinecone_manual(hadiths, INDEX_NAME)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ STORAGE COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüí° Next: Use the HadithRAG class for retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af70fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading hadiths from JSON...\n",
      "‚úÖ Loaded 92 hadiths\n",
      "üîÑ Loading embedding model...\n",
      "‚úÖ RAG system ready!\n",
      "\n",
      "üîé Query: Few people attacked the house of Lady Fatimah (SA) and burned the door\n",
      "\n",
      "üìö Retrieved 5 relevant hadiths\n",
      "\n",
      "======================================================================\n",
      "üìã RETRIEVED HADITHS:\n",
      "======================================================================\n",
      "\n",
      "  Hadith No. 1.0 (Pages 6-9)\n",
      "  Relevance: 0.5763\n",
      "  Preview: Sulaym says: ‚ÄúI heard Salman al-Farsi saying: ‚ÄúI was sitting with the Holy \n",
      "Prophet (SAW) while he was in that period of illness in which he passed \n",
      "a...\n",
      "\n",
      "  Hadith No. 67.0 (Pages 230-236)\n",
      "  Relevance: 0.5727\n",
      "  Preview: Sulaym says: ‚ÄúI was present at Ali (AS), at the time when Ziyad ibn ‚ÄòUbayd \n",
      "returned after having achieved victory in the Battle of Jamal. The house w...\n",
      "\n",
      "  Hadith No. 67.0 (Pages 230-236)\n",
      "  Relevance: 0.5727\n",
      "  Preview: Sulaym says: ‚ÄúI was present at Ali (AS), at the time when Ziyad ibn ‚ÄòUbayd \n",
      "returned after having achieved victory in the Battle of Jamal. The house w...\n",
      "\n",
      "  Hadith No. 67.0 (Pages 230-236)\n",
      "  Relevance: 0.5727\n",
      "  Preview: Sulaym says: ‚ÄúI was present at Ali (AS), at the time when Ziyad ibn ‚ÄòUbayd \n",
      "returned after having achieved victory in the Battle of Jamal. The house w...\n",
      "\n",
      "  Hadith No. 67.0 (Pages 230-236)\n",
      "  Relevance: 0.5727\n",
      "  Preview: Sulaym says: ‚ÄúI was present at Ali (AS), at the time when Ziyad ibn ‚ÄòUbayd \n",
      "returned after having achieved victory in the Battle of Jamal. The house w...\n",
      "\n",
      "======================================================================\n",
      "üìù FORMATTED CONTEXT (for LLM):\n",
      "======================================================================\n",
      "[Hadith No. 1.0, Pages 6-9]\n",
      "Sulaym says: ‚ÄúI heard Salman al-Farsi saying: ‚ÄúI was sitting with the Holy \n",
      "Prophet (SAW) while he was in that period of illness in which he passed \n",
      "away. Lady Fatimah (AS) entered and when she saw the weak state of the \n",
      "Holy Prophet‚Äôs (SAW) health, she got into the state of crying until tears \n",
      "started rolling down her cheeks. The Holy Prophet (SAW) asked: ‚ÄúWhy are \n",
      "you crying?‚Äù She replied ‚ÄúO Prophet of Allah after your death I am scared of \n",
      "destruction of myself and my children.‚Äù The Holy Prophet (SAW) with his \n",
      "eyes full of tears, said: ‚ÄúOh Fatimah don‚Äôt you know? We are people of that \n",
      "household which Allah has chosen for the hereafter instead of this world and \n",
      "He ordained it necessary for all beings to be destroyed. And Allah Tabarak \n",
      "wa T‚Äôala looked at the entire beings that He created and He selected me and \n",
      "\n",
      " \n",
      " \n",
      "made me a Nabi and a Rasul. Then He looked again and He chose your \n",
      "husband and commanded me to marry you to him and make him my caliph \n",
      "of...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone as PineconeBaseClient\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class HadithRAG:\n",
    "    def __init__(self, json_path='hadiths_updated.json'):\n",
    "        \"\"\"Initialize RAG system\"\"\"\n",
    "        \n",
    "        # Load full hadith texts from JSON\n",
    "        print(\"üìñ Loading hadiths from JSON...\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            hadiths_list = json.load(f)\n",
    "            # Create lookup dict by hadith_no\n",
    "            self.hadiths = {h['hadith_no']: h for h in hadiths_list}\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.hadiths)} hadiths\")\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        print(\"üîÑ Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='BAAI/bge-large-en-v1.5',\n",
    "            model_kwargs={\"device\": \"cuda\"}\n",
    "        )\n",
    "        \n",
    "        # Connect to Pinecone\n",
    "        pc = PineconeBaseClient(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "        self.index = pc.Index(os.getenv(\"PINECONE_INDEX_NAME_SQ_V2\"))\n",
    "        \n",
    "        print(\"‚úÖ RAG system ready!\")\n",
    "    \n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve relevant hadiths with full text\"\"\"\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Search Pinecone\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        # Lookup full texts from JSON\n",
    "        retrieved_hadiths = []\n",
    "        for match in results['matches']:\n",
    "            hadith_no = match['metadata']['hadith_no']\n",
    "            \n",
    "            # Get full hadith data from JSON\n",
    "            if hadith_no in self.hadiths:\n",
    "                hadith_data = self.hadiths[hadith_no]\n",
    "                \n",
    "                retrieved_hadiths.append({\n",
    "                    'hadith_no': hadith_no,\n",
    "                    'page_range': match['metadata'].get('page_range', 'N/A'),  # ‚Üê Use .get()\n",
    "                    'section_type': match['metadata'].get('section_type', 'hadith'),  # ‚Üê Use .get()\n",
    "                    'text': hadith_data['hadith_text'],\n",
    "                    'score': match['score']\n",
    "                })\n",
    "        \n",
    "        return retrieved_hadiths\n",
    "    \n",
    "    def format_context(self, retrieved_hadiths):\n",
    "        \"\"\"Format hadiths for LLM context\"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        for h in retrieved_hadiths:\n",
    "            if h['section_type'] == 'preface':\n",
    "                header = f\"[Preface, Pages {h['page_range']}]\"\n",
    "            else:\n",
    "                header = f\"[Hadith No. {h['hadith_no']}, Pages {h['page_range']}]\"\n",
    "            \n",
    "            formatted.append(f\"{header}\\n{h['text']}\\n---\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted)\n",
    "    \n",
    "    def query(self, question, k=10):\n",
    "        \"\"\"Complete query pipeline\"\"\"\n",
    "        print(f\"\\nüîé Query: {question}\\n\")\n",
    "        \n",
    "        # Retrieve relevant hadiths\n",
    "        retrieved = self.retrieve(question, k=k)\n",
    "        print(f\"üìö Retrieved {len(retrieved)} relevant hadiths\")\n",
    "        \n",
    "        # Format context\n",
    "        context = self.format_context(retrieved)\n",
    "        \n",
    "        return context, retrieved\n",
    "\n",
    "# Test usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG\n",
    "    rag = HadithRAG()\n",
    "    \n",
    "    # Test query\n",
    "    query = \"Few people attacked the house of Lady Fatimah (SA) and burned the door\"\n",
    "    context, results = rag.query(query, k=5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìã RETRIEVED HADITHS:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"\\n  Hadith No. {r['hadith_no']} (Pages {r['page_range']})\")\n",
    "        print(f\"  Relevance: {r['score']:.4f}\")\n",
    "        print(f\"  Preview: {r['text'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìù FORMATTED CONTEXT (for LLM):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(context[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f07107b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HadithRAG' from 'hadith_rag' (d:\\Sadiq\\rag-for-hq-books\\hadith_rag.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhadith_rag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HadithRAG\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HadithRAG' from 'hadith_rag' (d:\\Sadiq\\rag-for-hq-books\\hadith_rag.py)"
     ]
    }
   ],
   "source": [
    "from hadith_rag import HadithRAG\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from system_prompt import system_prompt as input_system_prompt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# System prompt\n",
    "system_prompt = input_system_prompt\n",
    "\n",
    "def get_answer(query, rag, model):\n",
    "    \"\"\"Get answer from RAG + Gemini\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    context, retrieved = rag.query(query, k=12)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = system_prompt.format(context=context, question=query)\n",
    "    \n",
    "    # Get response from Gemini\n",
    "    print(\"ü§î Generating response with Gemini...\\n\")\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return response.text, retrieved\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ INITIALIZING HADITH RAG SYSTEM WITH GEMINI\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Initialize RAG\n",
    "    rag = HadithRAG()\n",
    "    \n",
    "    # Initialize Gemini model\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"Tell me about Imam Ali talking to the sun as per what the Holy Prophet told him and how did the sun reply to Imam Ali. Also quote the entire incident and give hadith no and page number from the book.\",\n",
    "        \"Few people attacked the house of Lady Fatimah (SA) and burned the door. What was the entire incident? Explain in details. Also provide references like hadith no and page.\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Q: {query}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        answer, retrieved = get_answer(query, rag, model)\n",
    "        \n",
    "        print(\"RETRIEVED HADITHS:\")\n",
    "        for r in retrieved:\n",
    "            print(f\"  ‚Ä¢ Hadith No. {r['hadith_no']} (Pages {r['page_range']}) - Score: {r['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"ANSWER:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
